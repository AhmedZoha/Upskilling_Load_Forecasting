{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU09XtraWNsz"
      },
      "source": [
        "# Short term Load Forcasting (Regression Machine Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VcZ23QIWNs1"
      },
      "source": [
        "You have been provided with **Load Forecasting** data set as a single file, `dataset.csv`. \n",
        "\n",
        "We obtained it at http://www.mathworks.com/videos/electricity-load-and-price-forecasting-with-matlab-81765.html. \n",
        "\n",
        "For some background information you can also watch the video tutorial given in the link above:\n",
        "\n",
        "Before you start on this notebook, copy the datasetset `dataset.csv` in the same directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAi_QvBPWNs2"
      },
      "source": [
        "### 1. Set up notebook and load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZO5S3XoWNs3"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirement.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlODRJvSWNs4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model  # linear regression library\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFqjFFGuWNs4"
      },
      "source": [
        "This next snippet of code loads the load forecasting dataset. There are 50000 data points, each with 8 predictor variables that are input `x` and one response variable (which we'll denote `y`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cCZAmpoWWNs5"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('dataset.csv') # Reads the dataset in pandas dataframe\n",
        "\n",
        "features = ['DryBulb', 'DewPoint', 'Hour', 'Weekday', \n",
        "            'IsWorkingDay', 'P_W_S_hour_l', 'P_D_S_hour_l', 'prev24HrAveLoad']\n",
        "dataset.head() # This command tell the first 5 entries of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ba0JBUMWNs5"
      },
      "source": [
        "The dataset contains 8 input variable which contains the contextual information and historical load and temperature data. The variable includes:\n",
        "- DryBulb temperature\n",
        "- DewPoints\n",
        "- Hours (data is collected in hourly samples so each day has 24 values)\n",
        "- Weekday number [1 for Monday and 7 Sunday]\n",
        "- Holyday flag [IsWorkingDay 0 for working 1 for holiday]\n",
        "- prvious week same hour load (P_W_S_hour_l) 168 lagging entry i.e. 168 hours per week\n",
        "- prevuous day same hour load (P_D_S_hour_l) 24 lagging entry\n",
        "- prev24HrAveLoad (Average load for the perticular day)\n",
        "\n",
        "Target variable is actual load at that perticular hour that is need to be predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr1_iCP3WNs6"
      },
      "source": [
        "# Statistical Feature Extraction (Increase the Feature Space)\n",
        "This snippet of code extract the statistical features for the given data. These features includes the mean, standard deviation\n",
        "and variance of daily, 48 hours, 72 hours and weekly load. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeWZAGsVWNs6"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset['weekly_rollingavg'] = dataset['target_variable'].rolling(window=168).mean()\n",
        "dataset['weekly_std'] = dataset['target_variable'].rolling(window=168).std()\n",
        "dataset['daily_std'] = dataset['target_variable'].rolling(window=24).std()\n",
        "\n",
        "dataset['weekly_varience'] = dataset['target_variable'].rolling(window=168).var()\n",
        "dataset['daily_varience'] = dataset['target_variable'].rolling(window=24).var()\n",
        "\n",
        "\n",
        "dataset['weekly_median'] = dataset['target_variable'].rolling(window=168).median()\n",
        "dataset['daily_meadian'] = dataset['target_variable'].rolling(window=24).median()\n",
        "\n",
        "\n",
        "\n",
        "dataset['48_hour_mean'] = dataset['target_variable'].rolling(window=48).mean()\n",
        "dataset['48_hour_variance'] = dataset['target_variable'].rolling(window=48).var()\n",
        "dataset['48_hour_std'] = dataset['target_variable'].rolling(window=48).std()\n",
        "\n",
        "\n",
        "dataset['72_hour_mean'] = dataset['target_variable'].rolling(window=72).mean()\n",
        "dataset['72_hour_variance'] = dataset['target_variable'].rolling(window=72).var()\n",
        "dataset['72_hour_std'] = dataset['target_variable'].rolling(window=72).std()\n",
        "\n",
        "\n",
        "dataset['12_hours_temp_mean'] = dataset['DewPoint'].rolling(window=12).mean()\n",
        "dataset['24_hour_temp_mean'] = dataset['DewPoint'].rolling(window=24).mean()\n",
        "\n",
        "\n",
        "dataset['12_hours_temp_std'] = dataset['DewPoint'].rolling(window=12).std()\n",
        "dataset['24_hour_temp_std'] = dataset['DewPoint'].rolling(window=24).std()\n",
        "\n",
        "\n",
        "dataset['12_hours_temp_var'] = dataset['DewPoint'].rolling(window=12).var()\n",
        "dataset['24_hour_temp_var'] = dataset['DewPoint'].rolling(window=24).var()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh0ox7JSWNs6"
      },
      "outputs": [],
      "source": [
        "dataset.head() # Displays the first 5 values of expended dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94Ozd0OPWNs7"
      },
      "outputs": [],
      "source": [
        "variables = [*dataset]\n",
        "\n",
        "# The target_value is at index 8 so we have to remove it for the feature list.\n",
        "#\n",
        "\n",
        "index = 8\n",
        "features = variables[:index]+variables[index+1:] # remove the target variable for feature list\n",
        "print(features)\n",
        "\n",
        "print('----------------------------------------------')\n",
        "\n",
        "print('Number of features in training set:', len(features))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gbqp3-dWNs7"
      },
      "source": [
        "# Remove the missing value: (Data Filterig)\n",
        "\n",
        "- While generating feature, the first few values of each column has NaN (not a number) which requires filtering before model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqPhlMlDWNs7"
      },
      "outputs": [],
      "source": [
        "count=0\n",
        "for i in dataset.isnull().sum(axis=1):\n",
        "    if i>0:\n",
        "        count=count+1\n",
        "print('Total number of rows with missing values is ', count)\n",
        "#print('Since it is only',round((count/len(dataset.index))*100), 'percent of the entire dataset the rows with missing values are excluded.')\n",
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnopsxBSWNs7"
      },
      "source": [
        "### Some of the features have values that are need to be removed with NaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEuLO-u3WNs8"
      },
      "outputs": [],
      "source": [
        "# Removeing the NaN and checking the dataset set. This is the inital filtering of the data.\n",
        "dataset_new=dataset.dropna().reset_index(drop=True)\n",
        "dataset_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxNVEAcDWNs8"
      },
      "outputs": [],
      "source": [
        "count=0\n",
        "for i in dataset_new.isnull().sum(axis=1):\n",
        "    if i>0:\n",
        "        count=count+1\n",
        "print('Total number of rows with missing values is ', count)\n",
        "#print('Since it is only',round((count/len(dataset.index))*100), 'percent of the entire dataset the rows with missing values are excluded.')\n",
        "dataset_new.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HfiTr-QWNs8"
      },
      "source": [
        "# Check the shape of the current dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEmy2qTgWNs8"
      },
      "outputs": [],
      "source": [
        "print('The shape of the dataset:', dataset_new.shape)\n",
        "\n",
        "# out of 28, 27 are the input variables which includes the statisitcal and contextual features\n",
        "\n",
        "# the target value is the \"target_variable\" which is predicted using machine learning\n",
        "\n",
        "# The 'target_variable' is the actual hourly load on substation level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BWwSaImWNs8"
      },
      "source": [
        "# Preparing the Input and target values for model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLC4oJRhWNs8"
      },
      "outputs": [],
      "source": [
        "# Seperating the input and target variables for and converting them into numpy array.\n",
        "\n",
        "\n",
        "\n",
        "input_features = np.array(dataset_new.drop('target_variable', axis=1))         # Features\n",
        "target_values = np.array(dataset_new['target_variable'])                       # Output\n",
        "\n",
        "\n",
        "print('Shape of the Input data:', input_features.shape)\n",
        "print('Shape of the target variable:', target_values.shape)\n",
        "\n",
        "\n",
        "print('Data type of the input features:', input_features.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEpvnWL6WNs8"
      },
      "source": [
        " ## One feature Regression Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh7OGy5aWNs9"
      },
      "source": [
        "## Predict `y` using a single feature of `x`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7jAQAwTWNs9"
      },
      "source": [
        "Here we define a function, **one_feature_regression**, that takes `x` and `y`, along with the index `f` of a single feature and fits a linear regressor to `(x[f],y)`. It then plots the data along with the resulting line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIXDm49eWNs9"
      },
      "outputs": [],
      "source": [
        "def one_feature_regression(x,y,f):\n",
        "\n",
        "    '''\n",
        "    x -> one inpu feature\n",
        "\n",
        "    y -> Target value\n",
        "\n",
        "    f -> index of feature\n",
        "    '''\n",
        "\n",
        "\n",
        "    if (f < 0) or (f > 26):            # Because the total number of features in this dataset are 27\n",
        "        print(\"Feature index is out of bounds\")\n",
        "        return\n",
        "    regr = linear_model.LinearRegression()          # simple linear regressing model y=mx + c  where y output, x input, m slope, c intesecpt.\n",
        "    x1 = x[:,[f]]\n",
        "    regr.fit(x1, y)\n",
        "    # Make predictions using the model\n",
        "    y_pred = regr.predict(x1)\n",
        "    # Plot data points as well as predictions\n",
        "    plt.plot(x1, y, 'bo')\n",
        "    plt.plot(x1, y_pred, 'r-', linewidth=3)\n",
        "    plt.xlabel(features[f], fontsize=14)\n",
        "    plt.ylabel('Load Progression', fontsize=14)\n",
        "    plt.show()\n",
        "    print(\"Mean squared error: \", mean_squared_error(y, y_pred))     # Loss function which will be reduced using machine learning\n",
        "    print('Mean Absolute Percentage Errror', mean_absolute_percentage_error(y, y_pred)*100)\n",
        "    return regr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "911XMWuhWNs9"
      },
      "source": [
        "### (a) You have been given one_feature_regression function. Please change the index f to change the feature and develop a regression model. Compute the MSE, w, and b? (b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqgNV7GYWNs9"
      },
      "outputs": [],
      "source": [
        "# In the given example we are using the feature number 7 to train the regression model\n",
        "\n",
        "OneF_reg = one_feature_regression(input_features,target_values,7)    # Fearure is prev24HrAvgLoad\n",
        "\n",
        "\n",
        "w = OneF_reg.coef_\n",
        "b = OneF_reg.intercept_\n",
        "\n",
        "# Weights of regress\n",
        "\n",
        "print('Weights is %f, and Bais is %f', (w, b))    # 1 feature: 1 weight\n",
        "\n",
        "# This function provides the single feature regress along with the value of loss and MAPE. \n",
        "# The x-axis give the name of feature used for regression.\n",
        "\n",
        "\n",
        "# Find the one feature with mininmim MSE or MAPE. (Hint: simply substitute the f index in the above function calling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "DW5pG3rKWNs9"
      },
      "source": [
        "### (b) Identify the second-best feature with the lowest MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YItbJjYEWNs9"
      },
      "source": [
        "## Subset feature Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T2N6jCjWNs9"
      },
      "source": [
        "##  Predict `y` using a specified subset of features from `x`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2RFmpEqWNs9"
      },
      "source": [
        "The function **feature_subset_regression** is just like **one_feature_regression**, but this time uses a list of features `flist`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-5--C33WNs-"
      },
      "outputs": [],
      "source": [
        "def feature_subset_regression(x,y,flist):\n",
        "\n",
        "    '''\n",
        "    x -> input features\n",
        "    y -> target values\n",
        "\n",
        "    flist -> subset of feature for model training: pass list of the feature intex  [6,7] for feature number 7 and 8\n",
        "    '''\n",
        "\n",
        "\n",
        "    if len(flist) < 1:\n",
        "        print(\"Need at least one feature\")\n",
        "        return\n",
        "    for f in flist:\n",
        "        if (f < 0) or (f > 26):\n",
        "            print(\"Feature index is out of bounds\")\n",
        "            return\n",
        "    regr = linear_model.LinearRegression()\n",
        "    x1 = x[:,flist]\n",
        "    regr.fit(x1, y)\n",
        "    # Make predictions using the model\n",
        "    y_pred = regr.predict(x1)\n",
        "\n",
        "    print(\"Mean squared error: \", mean_squared_error(y, y_pred))\n",
        "    print('Mean Absolute Percentage Errror', mean_absolute_percentage_error(y, y_pred)*100)\n",
        "    return regr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GtAt-OvWNs-"
      },
      "source": [
        "### (a) Using the *feature_subset_regression*, find out the model performance in terms of MSE using features #7 (P_D_S_hour_l) and #8 (prev24HrAveLoad)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMKFHYSHWNs-"
      },
      "outputs": [],
      "source": [
        "# In this example we will use the feature number 6,7,8,9 and for regressing\n",
        "\n",
        "\n",
        "mul_f = feature_subset_regression(input_features, target_values, [5,6,7,8])   # rememeber index starts for 0\n",
        "\n",
        "# With multiple feature feature the loss function starts to decrease. \n",
        "\n",
        "\n",
        "w = mul_f.coef_\n",
        "b = mul_f.intercept_\n",
        "\n",
        "# Weights of regress\n",
        "\n",
        "print('Weights is %f, and Bais is %f', (w, b))   # four feature four weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWMxuRC1WNs-"
      },
      "source": [
        "### (b) Use all 27 Features for regression analysis and compare the model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgMcxkaCWNs-"
      },
      "outputs": [],
      "source": [
        "# Using all features for regression\n",
        "\n",
        "feature_list = [i for i in range(27)]\n",
        "\n",
        "reg = feature_subset_regression(input_features, target_values, feature_list)\n",
        "\n",
        "\n",
        "w = reg.coef_\n",
        "b = reg.intercept_\n",
        "\n",
        "# Weights of regress\n",
        "\n",
        "print('Length Weights is %f, and Bais is %f', (len(w), b))   # 27 feature four weights\n",
        "\n",
        "print('---------------------------------------------')\n",
        "\n",
        "print('Length Weights is %f, and Bais is %f', (w, b))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "A7sTLqV-WNs-"
      },
      "source": [
        "## Finding the best Train-Test Data Split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Cj6yC_WNs-"
      },
      "source": [
        "## Splitting the data into a training and test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWiPMvtyWNs-"
      },
      "source": [
        "In the experiments above, every model was fit to the *entire* data set and its mean squared error was evaluated on this same data set. This methodology would not, in general, yield accurate estimates of future error. In this specific case, however, the discrepancy might not be too bad because the data set is quite large relative to the number of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSfciGfdWNs-"
      },
      "source": [
        "To investigate this further, we define a procedure **split_data** that partitions the data set into separate training and test sets. It is invoked as follows:\n",
        "\n",
        "* `trainx, trainy, testx, testy = split_data(n_train)`\n",
        "\n",
        "Here:\n",
        "* `n_train` is the desired number of training points\n",
        "* `trainx` and `trainy` are the training points and response values\n",
        "* `testx` and `testy` are the test points and response values\n",
        "\n",
        "The split is done randomly, but the random seed is fixed, and thus the same split is produced if the procedure is called repeatedly with the same `n_train` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0cwQ8U5WNs_"
      },
      "outputs": [],
      "source": [
        "def split_data(n_train):\n",
        "    if (n_train < 0) or (n_train > 48000):\n",
        "        print(\"Invalid number of training points\")\n",
        "        return\n",
        "    np.random.seed(0)\n",
        "    perm = np.random.permutation(48000)\n",
        "    training_indices = perm[range(0,n_train)]\n",
        "    test_indices = perm[range(n_train,48000)]\n",
        "    trainx = input_features[training_indices,:]\n",
        "    trainy = target_values[training_indices]\n",
        "    testx = input_features[test_indices,:]\n",
        "    testy = target_values[test_indices]\n",
        "    return trainx, trainy, testx, testy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzr_fUJyWNs_"
      },
      "source": [
        "### (a)  Using the **split_data** procedure to partition the data set, compute the training MSE and test MSE when fitting a regressor to *all* features, for the following training set sizes:\n",
        "* `n_train = 20000`\n",
        "* `n_train = 25000`\n",
        "* `n_train = 30000`\n",
        "* `n_train = 40000`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQpX_OTOWNs_"
      },
      "outputs": [],
      "source": [
        "n_train = 40000\n",
        "\n",
        "# splitting the data\n",
        "xtrain, ytrain, xtest, ytest =split_data(n_train)\n",
        "\n",
        "# Generating the model\n",
        "reg = linear_model.LinearRegression()\n",
        "\n",
        "# Model training\n",
        "reg.fit(xtrain, ytrain)\n",
        "\n",
        "# Prediction\n",
        "\n",
        "y_pred = reg.predict(xtest)\n",
        "\n",
        "# MSE\n",
        "mse = mean_squared_error(ytest, y_pred)\n",
        "MAPE = mean_absolute_percentage_error(ytest, y_pred)*100\n",
        "\n",
        "print('For Training size=%d, the test MSE is :%f, and MAPE is :%f', n_train, mse, MAPE)\n",
        "\n",
        "\n",
        "# You can change the values of n_train and see the impact on MSE and MAPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEE9mBz0WNs_"
      },
      "source": [
        "### (b) What is the impact of increasing the training size on the performance of the model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NkX4JidWNs_"
      },
      "source": [
        "## Data Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMYSW3wzWNs_"
      },
      "source": [
        "###  (a) Apply Min-Max feature scaling to normalize the data and caluclate the MSE and MAPE on features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiKPozFqWNs_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "x_scaled=scaler.fit_transform(input_features)\n",
        "\n",
        "\n",
        "\n",
        "# training the model using scaled features\n",
        "\n",
        "reg_scaled = feature_subset_regression(x_scaled, target_values, feature_list) \n",
        "\n",
        "# function defined previously for using subset and all features for regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4MNqIvkWNs_"
      },
      "source": [
        "## Comparing Decision-Tree Versus Random Forest Regression Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTL6yz2NWNs_"
      },
      "source": [
        "### (a) Use test train split to divide the dataset in 70-30 and applying the decision tree and Random forest algorithms using sklearn library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K5EFJzvWNs_"
      },
      "source": [
        "Find the MSE and MAPE for both decision tree and random forest and compare it with linear regression and Plot the actual and predicted results of first 168 hours (weekly plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDlAHeiXWNtA"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries for decision tree and random forest for sklearn\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor  \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krn2-PmtWNtA"
      },
      "source": [
        "# Decision Tree Using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KJZdTzDWNtA"
      },
      "outputs": [],
      "source": [
        "# Creating the test train split using sklearn library\n",
        "\n",
        "# split is kept 70 - 30\n",
        "\n",
        "# the data is not shaffled\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_features,target_values, test_size=0.3, \n",
        "                                                        random_state=42, shuffle=False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRI1bs96WNtA"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "DecisionTreeRegressor(*, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, \n",
        "                        min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
        "                        max_features=None, random_state=None, max_leaf_nodes=None, \n",
        "                        min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
        "\n",
        "'''\n",
        "\n",
        "# creating a DF using the default parameters\n",
        "\n",
        "DT = DecisionTreeRegressor()    # Change parameter, write the parameter name and values in the function called\n",
        "\n",
        "# Model training\n",
        "\n",
        "start = time.time()\n",
        "DT = DT.fit(x_train, y_train)\n",
        "stop = time.time()\n",
        "\n",
        "dt_time = stop-start\n",
        "\n",
        "# model prediction\n",
        "\n",
        "y_pred_dt = DT.predict(x_test)\n",
        "\n",
        "# model evaluation\n",
        "\n",
        "print('Model Performance')\n",
        "\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "print('Decision Tree convergence time in seconds:', dt_time)\n",
        "\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "print('MSE:', mean_squared_error(y_test, y_pred_dt))\n",
        "print('MAPE:', mean_absolute_percentage_error(y_test, y_pred_dt)*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAUTNZacWNtA"
      },
      "source": [
        "To further optimise the model and improve performance, the parameters of the model are change.\n",
        "\n",
        "The process of obtaining the optimal model parameter is termed as hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKSAqVLvWNtA"
      },
      "source": [
        "# Weekly Plot of Decision Tree "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0Op9XPHWNtA"
      },
      "outputs": [],
      "source": [
        "# Plot the the results of actual vs predicted values\n",
        "\n",
        "plt.plot(y_test[1800:1968], 'b--', label='Actual')\n",
        "plt.plot(y_pred_dt[1800:1968], 'r-', label='Predicted')\n",
        "plt.title('Decision Tree Results')\n",
        "plt.xlabel('Hourly Time')\n",
        "plt.ylabel('Load in MW')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "figsize=(20,16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQwjNTnXWNtA"
      },
      "source": [
        "# Random Forest Using Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EajDUtxZWNtA"
      },
      "outputs": [],
      "source": [
        "''' \n",
        "RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, \n",
        "                    min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, \n",
        "                    min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, \n",
        "                    verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
        "'''\n",
        "\n",
        "\n",
        "# Parameters for random forest classifier\n",
        "# In this case, instead of using the default parameter, some of the parameter are been changed\n",
        "# Further, the parameter tuning is done using either grid search, random search or other techniques\n",
        "\n",
        "\n",
        "from random import random\n",
        "from tkinter import N\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# create the model\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=200, max_depth=30, n_jobs=-1, random_state=42)\n",
        "\n",
        "# train the model\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "stop = time.time()\n",
        "\n",
        "rf_time = stop-start\n",
        "# model predction\n",
        "\n",
        "y_pred_rf = rf.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "# model evaluation\n",
        "\n",
        "print('Model Performance')\n",
        "\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "print('Random Forest Convergence time in seconds:', rf_time)\n",
        "\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "print('MSE:', mean_squared_error(y_test, y_pred_rf))\n",
        "print('MAPE:', mean_absolute_percentage_error(y_test, y_pred_rf)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8EyfTI8WNtA"
      },
      "outputs": [],
      "source": [
        "# Plot the the results of actual vs predicted values\n",
        "\n",
        "plt.plot(y_test[1800:1968], 'b--', label='Actual')\n",
        "plt.plot(y_pred_rf[1800:1968], 'r-', label='Predicted')\n",
        "plt.title('Random Forest Results')\n",
        "plt.xlabel('Hourly Time')\n",
        "plt.ylabel('Load in MW')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "figsize=(20,16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSVAPaAEWNtB"
      },
      "source": [
        "For the above results, it is clearly evident that Random Forest performs better the DT and Linear Regression of this particular problem. \n",
        "\n",
        "The original dataset is expended to obtain 27 feature vecotors and analysis is done. Using linear regression, a short investigation is done on \n",
        "the impact of different feature set on the performace of model\n",
        "\n",
        "Decision Tress and Random Forest models are trained and evaluated using the 27 feature and results shows the performace of RF is better then DT.\n",
        "\n",
        "\n",
        "However, in machine learning the input feature set has a huge impact on the performance of the model. Therefore, we will perform an analysis on the\n",
        "feature importance and overall impact on the model performmance. Further, will smaller feature set the model convergence can be reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_59BBb2VWNtB"
      },
      "outputs": [],
      "source": [
        "import session_info\n",
        "session_info.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMsIs6POWNtB"
      },
      "outputs": [],
      "source": [
        "%load_ext watermark\n",
        "\n",
        "# python, ipython, packages, and machine characteristics\n",
        "%watermark -v -m -p wget,pandas,numpy,watermark,sklearn,matplotlib,keras\n",
        "\n",
        "# date\n",
        "print (\" \")\n",
        "%watermark -u -n -t -z "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X9aoO9LWNtB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "79fe433a8f88862a0a17970d6b2d3889ffdc46c96c7e93a37f3e8b4e3c05eef0"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}